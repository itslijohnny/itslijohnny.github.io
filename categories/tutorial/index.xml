<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tutorial on Johnny's Blog</title><link>https://johnnyli.cc/categories/tutorial/</link><description>Recent content in Tutorial on Johnny's Blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Johnny Li</copyright><lastBuildDate>Fri, 19 Mar 2021 19:56:44 -0700</lastBuildDate><atom:link href="https://johnnyli.cc/categories/tutorial/index.xml" rel="self" type="application/rss+xml"/><item><title>Pyforest - Automate Package Import Process for Your Data Science Project</title><link>https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/</link><pubDate>Fri, 19 Mar 2021 19:56:44 -0700</pubDate><guid>https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/</guid><description>&lt;img src="https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/j4dzCfs1YINnUeP.jpg" alt="Featured image of post Pyforest - Automate Package Import Process for Your Data Science Project" />&lt;p>Usually, the first step we all do for any Python project is importing required packages. So, the beginning of the file usually looks like this:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">xxxx&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">xxx&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">xxxx&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">xxxx&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Importing packages is one of my least favorite parts when I&amp;rsquo;m working on a Data Scientist project. Packages like &lt;em>pandas&lt;/em>, &lt;em>numpy&lt;/em>, &lt;em>lighteda&lt;/em>, &lt;em>matplotlib.pyplot&lt;/em>, and &lt;em>seaborn&lt;/em> are commonly used for my projects. I need to repeat this step in every single Jupyter Notebook. Moreover, when the analysis/research dives deeper, and I need to import new packages in the middle, I have to go back to the beginning and import new packages (If you are using Jupyter Notebook, don&amp;rsquo;t forget to press &lt;code>Ctrl&lt;/code> + &lt;code>Enter&lt;/code>).&lt;/p>
&lt;p>&lt;img src="https://i.loli.net/2021/03/20/mle46dnAb8zsr1K.gif"
loading="lazy"
alt="Import in the middle. "
>
&lt;p style="font-size:1.4rem;text-align:center; margin-top: -2.8rem; color: var(--card-text-color-secondary);">Going back and forth to import packages is frustrating.&lt;/p>&lt;/p>
&lt;p>I can reduce some repetitive works by using template tools like cookiecutter.&lt;/p>
&lt;div class="gallery">
&lt;p>
&lt;a href="https://github.com/itslijohnny/cookiecutter_data_analysis">&lt;img src="https://gh-card.dev/repos/itslijohnny/cookiecutter_data_analysis.svg?fullname=" loading='lazy'>&lt;/a>
&lt;/p>
&lt;/div>
&lt;p>But, is there a way to automatically import related packages and add import statements for me, like this?
&lt;img src="https://github.com/8080labs/pyforest/raw/master/examples/assets/pyforest_demo_in_jupyter_notebook.gif"
loading="lazy"
>
&lt;p style="font-size:1.4rem;text-align:center; margin-top: -2.8rem; color: var(--card-text-color-secondary);">The packages are automatically imported in the first cell.&lt;/p>&lt;/p>
&lt;p>But how? &lt;strong>Pyforest&lt;/strong> - a package can automatically import packages for you.
&lt;div class="gallery">
&lt;p>
&lt;a href="https://github.com/8080labs/pyforest">&lt;img src="https://gh-card.dev/repos/8080labs/pyforest.svg?fullname=" loading='lazy'>&lt;/a>
&lt;/p>
&lt;/div>&lt;/p>
&lt;p>Pyforest can &amp;ldquo;lazy&amp;rdquo; import the packages and add the import statement to your Jupyter Notebook. It will not import packages you are not using.&lt;/p>
&lt;p>You can install it using pip:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-shell" data-lang="shell">pip install --upgrade pyforest
python -m pyforest install_extensions
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>The installation will add pyforest_autoimport.py to your Jupyter and IPython default startup settings. Therefore, you can start coding in Jupyter or Ipython and don&amp;rsquo;t need to import pyforest.&lt;/p>
&lt;p>&lt;img src="https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/pyforestdemo.png"
width="606"
height="166"
srcset="https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/pyforestdemo_hu8584359d6e9be33951add4cb35ae0fa4_9830_480x0_resize_box_3.png 480w, https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/pyforestdemo_hu8584359d6e9be33951add4cb35ae0fa4_9830_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="I didn&amp;rsquo;t add the statement for import pandas as pd."
class="gallery-image"
data-flex-grow="365"
data-flex-basis="876px"
>&lt;/p>
&lt;p>Also, you can notice that I didn&amp;rsquo;t type the full name of pandas library. Pyforest has already included all the common abbreviations.
&lt;img src="https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/sourcecode.png"
width="491"
height="392"
srcset="https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/sourcecode_hudfe07432bc95aaaf3409e0bad5f8f1fc_25646_480x0_resize_box_3.png 480w, https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/sourcecode_hudfe07432bc95aaaf3409e0bad5f8f1fc_25646_1024x0_resize_box_3.png 1024w"
loading="lazy"
alt="Pyforest includes many import statements."
class="gallery-image"
data-flex-grow="125"
data-flex-basis="300px"
>&lt;/p>
&lt;p>You can also edit ~/.pyforest/user_imports.py to add your customized import statements.&lt;/p>
&lt;p>&lt;img src="https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/user_imports.png"
width="623"
height="394"
srcset="https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/user_imports_hucf6f272c83a09cb13413cf5e43137c2f_32185_480x0_resize_box_3.png 480w, https://johnnyli.cc/p/pyforest-automate-package-import-process-for-your-data-science-project/user_imports_hucf6f272c83a09cb13413cf5e43137c2f_32185_1024x0_resize_box_3.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="158"
data-flex-basis="379px"
>&lt;/p>
&lt;p>Pyforest works better on Jupyter Notebook or IPython. If you want this feature to work in other editors, you can import Pyforest first. e.g.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">pyforest&lt;/span>
&lt;span class="n">pyforest&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">active_imports&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This is my daily dose of Python tricks. I hope this post can help you. I strongly recommend you try &lt;a class="link" href="https://github.com/itslijohnny/cookiecutter_data_analysis" target="_blank" rel="noopener"
>cookiecutter&lt;/a> and &lt;a class="link" href="https://github.com/8080labs/pyforest" target="_blank" rel="noopener"
>Pyforest&lt;/a> to automate your Data Science/Analysis workflow.&lt;/p>
&lt;hr>
&lt;style>
.resp-sharing-button__link,
.resp-sharing-button__icon {
display: inline-block
}
.resp-sharing-button__link {
text-decoration: none;
color: #fff;
margin: 0.5em
}
.resp-sharing-button {
border-radius: 5px;
transition: 25ms ease-out;
padding: 0.5em 0.75em;
font-family: Helvetica Neue,Helvetica,Arial,sans-serif
}
.resp-sharing-button__icon svg {
width: 1em;
height: 1em;
margin-right: 0.4em;
vertical-align: top
}
.resp-sharing-button--small svg {
margin: 0;
vertical-align: middle
}
.resp-sharing-button__icon {
stroke: #fff;
fill: none
}
.resp-sharing-button__icon--solid,
.resp-sharing-button__icon--solidcircle {
fill: #fff;
stroke: none
}
.resp-sharing-button--twitter {
background-color: #55acee
}
.resp-sharing-button--twitter:hover {
background-color: #2795e9
}
.resp-sharing-button--pinterest {
background-color: #bd081c
}
.resp-sharing-button--pinterest:hover {
background-color: #8c0615
}
.resp-sharing-button--facebook {
background-color: #3b5998
}
.resp-sharing-button--facebook:hover {
background-color: #2d4373
}
.resp-sharing-button--tumblr {
background-color: #35465C
}
.resp-sharing-button--tumblr:hover {
background-color: #222d3c
}
.resp-sharing-button--reddit {
background-color: #5f99cf
}
.resp-sharing-button--reddit:hover {
background-color: #3a80c1
}
.resp-sharing-button--google {
background-color: #dd4b39
}
.resp-sharing-button--google:hover {
background-color: #c23321
}
.resp-sharing-button--linkedin {
background-color: #0077b5
}
.resp-sharing-button--linkedin:hover {
background-color: #046293
}
.resp-sharing-button--email {
background-color: #777
}
.resp-sharing-button--email:hover {
background-color: #5e5e5e
}
.resp-sharing-button--xing {
background-color: #1a7576
}
.resp-sharing-button--xing:hover {
background-color: #114c4c
}
.resp-sharing-button--whatsapp {
background-color: #25D366
}
.resp-sharing-button--whatsapp:hover {
background-color: #1da851
}
.resp-sharing-button--hackernews {
background-color: #FF6600
}
.resp-sharing-button--hackernews:hover, .resp-sharing-button--hackernews:focus { background-color: #FB6200 }
.resp-sharing-button--vk {
background-color: #507299
}
.resp-sharing-button--vk:hover {
background-color: #43648c
}
.resp-sharing-button--facebook {
background-color: #3b5998;
border-color: #3b5998;
}
.resp-sharing-button--facebook:hover,
.resp-sharing-button--facebook:active {
background-color: #2d4373;
border-color: #2d4373;
}
.resp-sharing-button--twitter {
background-color: #55acee;
border-color: #55acee;
}
.resp-sharing-button--twitter:hover,
.resp-sharing-button--twitter:active {
background-color: #2795e9;
border-color: #2795e9;
}
.resp-sharing-button--email {
background-color: #777777;
border-color: #777777;
}
.resp-sharing-button--email:hover,
.resp-sharing-button--email:active {
background-color: #5e5e5e;
border-color: #5e5e5e;
}
.resp-sharing-button--linkedin {
background-color: #0077b5;
border-color: #0077b5;
}
.resp-sharing-button--linkedin:hover,
.resp-sharing-button--linkedin:active {
background-color: #046293;
border-color: #046293;
}
.resp-sharing-button--reddit {
background-color: #5f99cf;
border-color: #5f99cf;
}
.resp-sharing-button--reddit:hover,
.resp-sharing-button--reddit:active {
background-color: #3a80c1;
border-color: #3a80c1;
}
.resp-sharing-button--telegram {
background-color: #54A9EB;
}
.resp-sharing-button--telegram:hover {
background-color: #4B97D1;}
.share {
margin: 1.5em 0;
padding: 0 var(--card-padding);
font-size: 0.8em;
}
.share svg{
margin-top: 0.3em;
}
.share a:hover{
color:#dddddd;
}
.share p{
font-size: 1.3em;
margin:0 auto;
}
&lt;/style>
&lt;div class="share">
&lt;p>If you think your friends/network would find this useful, please share it with them – I’d really appreciate it.&lt;/p>
&lt;a class="resp-sharing-button__link" href="https://facebook.com/sharer/sharer.php?u=http%3A%2F%2Flijohnny.com%2Fp%2Fpyforest-automate-package-import-process-for-your-data-science-project" target="_blank" rel="noopener" aria-label="Facebook" style="margin-left: -0.18em;">
&lt;div class="resp-sharing-button resp-sharing-button--facebook resp-sharing-button--medium">&lt;div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path d="M18.77 7.46H14.5v-1.9c0-.9.6-1.1 1-1.1h3V.5h-4.33C10.24.5 9.5 3.44 9.5 5.32v2.15h-3v4h3v12h5v-12h3.85l.42-4z"/>&lt;/svg>&lt;/div>Facebook&lt;/div>
&lt;/a>
&lt;a class="resp-sharing-button__link" href="https://twitter.com/intent/tweet/?text=Pyforest%20-%20Automate%20Package%20Import%20Process%20for%20Your%20Data%20Science%20Project&amp;amp;url=http%3A%2F%2Flijohnny.com%2Fp%2Fpyforest-automate-package-import-process-for-your-data-science-project" target="_blank" rel="noopener" aria-label="Twitter">
&lt;div class="resp-sharing-button resp-sharing-button--twitter resp-sharing-button--medium">&lt;div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path d="M23.44 4.83c-.8.37-1.5.38-2.22.02.93-.56.98-.96 1.32-2.02-.88.52-1.86.9-2.9 1.1-.82-.88-2-1.43-3.3-1.43-2.5 0-4.55 2.04-4.55 4.54 0 .36.03.7.1 1.04-3.77-.2-7.12-2-9.36-4.75-.4.67-.6 1.45-.6 2.3 0 1.56.8 2.95 2 3.77-.74-.03-1.44-.23-2.05-.57v.06c0 2.2 1.56 4.03 3.64 4.44-.67.2-1.37.2-2.06.08.58 1.8 2.26 3.12 4.25 3.16C5.78 18.1 3.37 18.74 1 18.46c2 1.3 4.4 2.04 6.97 2.04 8.35 0 12.92-6.92 12.92-12.93 0-.2 0-.4-.02-.6.9-.63 1.96-1.22 2.56-2.14z"/>&lt;/svg>&lt;/div>Twitter&lt;/div>
&lt;/a>
&lt;a class="resp-sharing-button__link" href="mailto:?subject=Pyforest%20-%20Automate%20Package%20Import%20Process%20for%20Your%20Data%20Science%20Project&amp;amp;body=http%3A%2F%2Flijohnny.com%2Fp%2Fpyforest-automate-package-import-process-for-your-data-science-project" target="_self" rel="noopener" aria-label="E-Mail">
&lt;div class="resp-sharing-button resp-sharing-button--email resp-sharing-button--medium">&lt;div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path d="M22 4H2C.9 4 0 4.9 0 6v12c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zM7.25 14.43l-3.5 2c-.08.05-.17.07-.25.07-.17 0-.34-.1-.43-.25-.14-.24-.06-.55.18-.68l3.5-2c.24-.14.55-.06.68.18.14.24.06.55-.18.68zm4.75.07c-.1 0-.2-.03-.27-.08l-8.5-5.5c-.23-.15-.3-.46-.15-.7.15-.22.46-.3.7-.14L12 13.4l8.23-5.32c.23-.15.54-.08.7.15.14.23.07.54-.16.7l-8.5 5.5c-.08.04-.17.07-.27.07zm8.93 1.75c-.1.16-.26.25-.43.25-.08 0-.17-.02-.25-.07l-3.5-2c-.24-.13-.32-.44-.18-.68s.44-.32.68-.18l3.5 2c.24.13.32.44.18.68z"/>&lt;/svg>&lt;/div>E-Mail&lt;/div>
&lt;/a>
&lt;a class="resp-sharing-button__link" href="https://www.linkedin.com/shareArticle?mini=true&amp;amp;url=http%3A%2F%2Flijohnny.com%2Fp%2Fpyforest-automate-package-import-process-for-your-data-science-project&amp;amp;title=Pyforest%20-%20Automate%20Package%20Import%20Process%20for%20Your%20Data%20Science%20Project&amp;amp;summary=Pyforest%20-%20Automate%20Package%20Import%20Process%20for%20Your%20Data%20Science%20Project&amp;amp;source=http%3A%2F%2Flijohnny.com%2Fp%2Fpyforest-automate-package-import-process-for-your-data-science-project" target="_blank" rel="noopener" aria-label="LinkedIn">
&lt;div class="resp-sharing-button resp-sharing-button--linkedin resp-sharing-button--medium">&lt;div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path d="M6.5 21.5h-5v-13h5v13zM4 6.5C2.5 6.5 1.5 5.3 1.5 4s1-2.4 2.5-2.4c1.6 0 2.5 1 2.6 2.5 0 1.4-1 2.5-2.6 2.5zm11.5 6c-1 0-2 1-2 2v7h-5v-13h5V10s1.6-1.5 4-1.5c3 0 5 2.2 5 6.3v6.7h-5v-7c0-1-1-2-2-2z"/>&lt;/svg>&lt;/div>LinkedIn&lt;/div>
&lt;/a>
&lt;a class="resp-sharing-button__link" href="https://reddit.com/submit/?url=http%3A%2F%2Flijohnny.com%2Fp%2Fpyforest-automate-package-import-process-for-your-data-science-project&amp;amp;resubmit=true&amp;amp;title=Pyforest%20-%20Automate%20Package%20Import%20Process%20for%20Your%20Data%20Science%20Project" target="_blank" rel="noopener" aria-label="Reddit">
&lt;div class="resp-sharing-button resp-sharing-button--reddit resp-sharing-button--medium">&lt;div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path d="M24 11.5c0-1.65-1.35-3-3-3-.96 0-1.86.48-2.42 1.24-1.64-1-3.75-1.64-6.07-1.72.08-1.1.4-3.05 1.52-3.7.72-.4 1.73-.24 3 .5C17.2 6.3 18.46 7.5 20 7.5c1.65 0 3-1.35 3-3s-1.35-3-3-3c-1.38 0-2.54.94-2.88 2.22-1.43-.72-2.64-.8-3.6-.25-1.64.94-1.95 3.47-2 4.55-2.33.08-4.45.7-6.1 1.72C4.86 8.98 3.96 8.5 3 8.5c-1.65 0-3 1.35-3 3 0 1.32.84 2.44 2.05 2.84-.03.22-.05.44-.05.66 0 3.86 4.5 7 10 7s10-3.14 10-7c0-.22-.02-.44-.05-.66 1.2-.4 2.05-1.54 2.05-2.84zM2.3 13.37C1.5 13.07 1 12.35 1 11.5c0-1.1.9-2 2-2 .64 0 1.22.32 1.6.82-1.1.85-1.92 1.9-2.3 3.05zm3.7.13c0-1.1.9-2 2-2s2 .9 2 2-.9 2-2 2-2-.9-2-2zm9.8 4.8c-1.08.63-2.42.96-3.8.96-1.4 0-2.74-.34-3.8-.95-.24-.13-.32-.44-.2-.68.15-.24.46-.32.7-.18 1.83 1.06 4.76 1.06 6.6 0 .23-.13.53-.05.67.2.14.23.06.54-.18.67zm.2-2.8c-1.1 0-2-.9-2-2s.9-2 2-2 2 .9 2 2-.9 2-2 2zm5.7-2.13c-.38-1.16-1.2-2.2-2.3-3.05.38-.5.97-.82 1.6-.82 1.1 0 2 .9 2 2 0 .84-.53 1.57-1.3 1.87z"/>&lt;/svg>&lt;/div>Reddit&lt;/div>
&lt;/a>
&lt;a class="resp-sharing-button__link" href="https://telegram.me/share/url?text=Pyforest%20-%20Automate%20Package%20Import%20Process%20for%20Your%20Data%20Science%20Project&amp;amp;url=http%3A%2F%2Flijohnny.com%2Fp%2Fpyforest-automate-package-import-process-for-your-data-science-project" target="_blank" rel="noopener" aria-label="Telegram">
&lt;div class="resp-sharing-button resp-sharing-button--telegram resp-sharing-button--medium">&lt;div aria-hidden="true" class="resp-sharing-button__icon resp-sharing-button__icon--solid">
&lt;svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">&lt;path d="M.707 8.475C.275 8.64 0 9.508 0 9.508s.284.867.718 1.03l5.09 1.897 1.986 6.38a1.102 1.102 0 0 0 1.75.527l2.96-2.41a.405.405 0 0 1 .494-.013l5.34 3.87a1.1 1.1 0 0 0 1.046.135 1.1 1.1 0 0 0 .682-.803l3.91-18.795A1.102 1.102 0 0 0 22.5.075L.706 8.475z"/>&lt;/svg>&lt;/div>Telegram&lt;/div>
&lt;/a>
&lt;/div>
&lt;hr></description></item><item><title>Craigslist Vacation House Data Crawling</title><link>https://johnnyli.cc/p/craigslist-vacation-house-data-crawling/</link><pubDate>Tue, 12 Feb 2019 01:13:34 -0800</pubDate><guid>https://johnnyli.cc/p/craigslist-vacation-house-data-crawling/</guid><description>&lt;img src="https://johnnyli.cc/p/craigslist-vacation-house-data-crawling/5cd7d789264a1.jpg" alt="Featured image of post Craigslist Vacation House Data Crawling" />&lt;p>In this &lt;a class="link" href="https://github.com/itslijohnny/web-crawler-tutorial/tree/master/scrapy_craigslist" target="_blank" rel="noopener"
>tutorial&lt;/a> I use &lt;a class="link" href="https://scrapy.org/" target="_blank" rel="noopener"
>Scrapy&lt;/a> to collect data from Craigslist.com. Specifically, the data under craigslist.org/Seattle/housing/vacation rentals. You can find the page under the link: &lt;a class="link" href="https://seattle.craigslist.org/d/vacation%e2%80%90rentals/search/vac" target="_blank" rel="noopener"
>https://seattle.craigslist.org/d/vacation‐rentals/search/vac&lt;/a>&lt;/p>
&lt;p>In the example, I collected following information:&lt;/p>
&lt;ol>
&lt;li>Title&lt;/li>
&lt;li>Posted Date&lt;/li>
&lt;li>Rental Price&lt;/li>
&lt;li>Number of bedrooms&lt;/li>
&lt;li>Neighborhood&lt;/li>
&lt;li>Description&lt;/li>
&lt;/ol>
&lt;blockquote>
&lt;p>For more information or the code, please go to my &lt;a class="link" href="https://github.com/itslijohnny/web-crawler-tutorial/tree/master/scrapy_craigslist" target="_blank" rel="noopener"
>github page&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;h2 id="preparation">PREPARATION&lt;/h2>
&lt;h3 id="installation">INSTALLATION&lt;/h3>
&lt;p>You can install scrapy through &lt;code>pip install&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ pip install scrapy
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>or use &lt;code>conda install&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ conda install scrapy
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="creat-project">CREAT PROJECT&lt;/h3>
&lt;p>Before we start coding, we can use &lt;code>scrapy startproject&lt;/code> command to quickly create a project.
In terminal or CMD, navigate to your desired folder and execute following command:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ scrapy startproject scrapy_craigslist
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Here &lt;em>scrapy_craigslist&lt;/em> is the name of the project.&lt;/p>
&lt;p>After that, we can use &lt;code>genspider&lt;/code> command to create a Scrapy Spider. Here, we name it vacation_rentals and designated a URL. We user craiglist.org Seattle vacation house list page as an example.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ scrapy genspider vacation_rentals seattle.craigslist.org/d/vacation‐rentals/search/vac
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>This will create a directory with the following structure:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-fallback" data-lang="fallback">─── scrapy_craigslist
├── __init__.py
├── __pycache__
│   ├── __init__.cpython-36.pyc
│   └── settings.cpython-36.pyc
├── items.py
├── middlewares.py
├── pipelines.py
├── settings.py
└── spiders
├── __init__.py
├── __pycache__
│   ├── __init__.cpython-36.pyc
│   └── vacation_rentals.cpython-36.pyc
└── vacation_rentals.py
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="editing">EDITING&lt;/h2>
&lt;p>Navigate to the spiders folder and open the spider py file in your favorite editor.
There are some pre written code, but you need to make sure that &lt;code>allowed_domains&lt;/code> and &lt;code>start_urls&lt;/code> are in the right form.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="kn">import&lt;/span> &lt;span class="nn">scrapy&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">CarSpider&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scrapy&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Spider&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;car&amp;#39;&lt;/span>
&lt;span class="n">allowed_domains&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;craigslist.org&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">start_urls&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;https://seattle.craigslist.org/d/vacation‐rentals/search/vac/&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">parse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="k">pass&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>Let&amp;rsquo;s write our own code under &lt;code>def parse(self, response):&lt;/code>. You can check the code &lt;a class="link" href="https://github.com/itslijohnny/web-crawler-tutorial/blob/master/scrapy_craigslist/scrapy_craigslist/spiders/vacation_rentals.py" target="_blank" rel="noopener"
>here&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="c1"># -*- coding: utf-8 -*-&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">scrapy&lt;/span>
&lt;span class="kn">from&lt;/span> &lt;span class="nn">scrapy&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">Request&lt;/span>
&lt;span class="kn">import&lt;/span> &lt;span class="nn">re&lt;/span>
&lt;span class="k">class&lt;/span> &lt;span class="nc">VacationRentalsSpider&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">scrapy&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Spider&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="n">name&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s1">&amp;#39;vacation_rentals&amp;#39;&lt;/span>
&lt;span class="n">allowed_domains&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;craigslist.org&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="n">start_urls&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s1">&amp;#39;http://seattle.craigslist.org/d/vacation‐rentals/search/vac/&amp;#39;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">parse&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="c1"># Extract all wrapper for each list item between &amp;lt;p class=&amp;#34;result-info&amp;#34;&amp;gt;&amp;lt;/p&amp;gt;&lt;/span>
&lt;span class="n">vacs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;//p[@class=&amp;#34;result-info&amp;#34;]&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># Get next page button URL &amp;lt;a href=&amp;#34;/search/vac?s=120&amp;#34; class=&amp;#34;button next&amp;#34; title=&amp;#34;next page&amp;#34;&amp;gt;next &amp;amp;gt; &amp;lt;/a&amp;gt;&lt;/span>
&lt;span class="n">next_rel_url&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;//a[@class=&amp;#34;button next&amp;#34;]/@href&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extract_first&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1"># Get full address.&lt;/span>
&lt;span class="n">next_url&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">urljoin&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_rel_url&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># Go through all the pages.&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">Request&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">next_url&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">callback&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parse&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># Loop each item to extract title, posted date, rental price, number of bedrooms, and neighborhood&lt;/span>
&lt;span class="k">for&lt;/span> &lt;span class="n">vac&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">vacs&lt;/span>&lt;span class="p">:&lt;/span>
&lt;span class="c1"># Get title from &amp;lt;a&amp;gt;&amp;lt;/a&amp;gt; tag.&lt;/span>
&lt;span class="n">title&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">vac&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;a/text()&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extract_first&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1"># Get posted date from &amp;lt;time class=&amp;#34;result-date&amp;#34; datetime=&amp;#34;2019-03-06 18:34&amp;#34; title=&amp;#34;Wed 06 Mar 06:34:28 PM&amp;#34;&amp;gt;Mar 6&amp;lt;/time&amp;gt; block. Use @datetime for attribute datetime.&lt;/span>
&lt;span class="n">pdate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">vac&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;time/@datetime&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extract_first&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">()[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>
&lt;span class="c1"># Get rental price form &amp;lt;span class=&amp;#34;result-price&amp;#34;&amp;gt;$84&amp;lt;/span&amp;gt;&lt;/span>
&lt;span class="n">rprice&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">vac&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;span/span[@class=&amp;#34;result-price&amp;#34;]/text()&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extract_first&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1"># Get Number of bedrooms from &amp;lt;span class=&amp;#34;housing&amp;#34;&amp;gt;2br - 760ft&amp;lt;sup&amp;gt;2&amp;lt;/sup&amp;gt; - &amp;lt;/span&amp;gt; and clean up the extra&lt;/span>
&lt;span class="n">nbedroom&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vac&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;span/span[@class=&amp;#34;housing&amp;#34;]/text()&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extract_first&lt;/span>&lt;span class="p">())&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;-&amp;#39;&lt;/span>&lt;span class="p">)[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">strip&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1"># Get Neighborhood from &amp;lt;span class=&amp;#34;result-hood&amp;#34;&amp;gt; (*** - *****)&amp;lt;/span&amp;gt;&lt;/span>
&lt;span class="n">hood&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">re&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sub&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;[()]&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">str&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vac&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;span/span[@class=&amp;#34;result-hood&amp;#34;]/text()&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extract_first&lt;/span>&lt;span class="p">()))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">strip&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1"># Get the address of description page of each vacation house.&lt;/span>
&lt;span class="n">vacaddress&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">vac&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;a/@href&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extract_first&lt;/span>&lt;span class="p">()&lt;/span>
&lt;span class="c1"># We needed open the URL of each house and scrape the house description, while passing the meta to parse_page function.&lt;/span>
&lt;span class="k">yield&lt;/span> &lt;span class="n">Request&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vacaddress&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">callback&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parse_page&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">meta&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;URL&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">vacaddress&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;Title&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">title&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;Posted Date&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">pdate&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s2">&amp;#34;Rental Price&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">rprice&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s2">&amp;#34;Number of bedrooms&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">nbedroom&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;Neighborhood&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">hood&lt;/span>&lt;span class="p">})&lt;/span>
&lt;span class="c1"># Extract description page of the vacation house.&lt;/span>
&lt;span class="k">def&lt;/span> &lt;span class="nf">parse_page&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="p">):&lt;/span>
&lt;span class="c1"># Pass the variables&lt;/span>
&lt;span class="n">url&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">meta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;URL&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">title&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">meta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Title&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">pdate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">meta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Posted Date&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">rprice&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">meta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Rental Price&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">nbedroom&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">meta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Number of bedrooms&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="n">hood&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">meta&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Neighborhood&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;span class="c1"># Get the description.&lt;/span>
&lt;span class="n">description&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">join&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">line&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">line&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">response&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">xpath&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;//*[@id=&amp;#34;postingbody&amp;#34;]/text()&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">extract&lt;/span>&lt;span class="p">())&lt;/span>
&lt;span class="k">yield&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="s1">&amp;#39;Title&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">title&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;Posted Date&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">pdate&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s2">&amp;#34;Rental Price&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">rprice&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s2">&amp;#34;Number of bedrooms&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">nbedroom&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;Neighborhood&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">hood&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;Description&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">description&lt;/span>&lt;span class="p">}&lt;/span>
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="run-spider">RUN SPIDER&lt;/h3>
&lt;p>To put our spider to work, run &lt;code>crawl&lt;/code> command in terminal or CMD:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">$ scrapy crawl vacation_rentals -o result-titles.csv
&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>&lt;code>-o&lt;/code> means out put data into file. &lt;code>result-titles.csv&lt;/code> is the files' name.&lt;/p></description></item></channel></rss>